{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from models.experimental import attempt_load\n",
    "import cv2\n",
    "from utils.datasets import letterbox\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import time\n",
    "from utils.general import non_max_suppression_kpt,strip_optimizer,xyxy2xywh\n",
    "from utils.plots import output_to_keypoint, plot_skeleton_kpts,colors,plot_one_box_kpt\n",
    "import logging\n",
    "import pandas as pd\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device ='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_path = 'yolov7-w6-pose.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_model(device, model_path):\n",
    "    x = torch.load(model_path, map_location=torch.device(device))\n",
    "\n",
    "    if x.get('ema'):\n",
    "        x['model'] = x['ema']  # replace model with ema\n",
    "    for k in 'optimizer', 'training_results', 'wandb_id', 'ema', 'updates':  # keys\n",
    "        x[k] = None\n",
    "    x['epoch'] = -1\n",
    "    if device!='cpu':\n",
    "        x['model'].half()  # to FP16\n",
    "    else:\n",
    "        x['model'].float()\n",
    "    for p in x['model'].parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    torch.save(x, model_path)\n",
    "    mb = os.path.getsize(model_path) / 1E6  # filesize\n",
    "    print(f\"Optimizer stripped from {model_path},{(f' saved as {model_path},') if model_path else ''} {mb:.1f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer stripped from yolov7-w6-pose.pt, saved as yolov7-w6-pose.pt, 161.1MB\n",
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g/.local/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "strip_model(device, model_path)\n",
    "model = attempt_load(model_path, map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Things to identify: ['person']\n"
     ]
    }
   ],
   "source": [
    "_ = model.eval()\n",
    "names = model.module.names if hasattr(model, 'module') else model.names  # get class names\n",
    "print(f\"Things to identify: {names}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To open Video capture:\n",
    " - integer that represents a webcam\n",
    " - path to a video file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_source(source):\n",
    "    if source.isnumeric() :    \n",
    "        cap = cv2.VideoCapture(int(source))    #pass video to videocapture object\n",
    "    else :\n",
    "        cap = cv2.VideoCapture(source)    #pass video to videocapture object\n",
    "    if cap.isOpened() == False:   #check if videocapture not opened\n",
    "        print('Source not found. Check path')\n",
    "    else:\n",
    "        frame_width = int(cap.get(3))  #get video frame width\n",
    "        frame_height = int(cap.get(4)) #get video frame height\n",
    "    return cap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating over frames\n",
    "For the process of retrieving sequences of landmarks, we will have a `sequence_length` which is the amount of frames taken into consideration for a single sequence of landmark. And also we will have a `separation` which is the amount of frames *ignored* between one set of landmarks and another inside one sequence of landmarks.\n",
    "\n",
    "We will capture landmarks every `separation` and from those captured we will create `N` arrays of length `sequence_length` containing those landmarks \n",
    "\n",
    "### How will we identify the same person in every iteration\n",
    "First, identify the object with more landmarks identified and store that set of landmarks in `base_landmarks`. Then considering the distance between `base_landmarks` and the other objects identified in the next objects identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sequence row:\n",
    "    Video Timestamp Set of landmarks \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def landmarks_sequence_for_video(video_path, sequence_length=10, separation=6):\n",
    "    \"\"\"\n",
    "        Args\n",
    "        Returns\n",
    "    \"\"\"\n",
    "    sequence_length = sequence_length\n",
    "    separation = separation\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    sequences = [[]]\n",
    "\n",
    "    cap = load_source(video_path)\n",
    "    frame_width = int(cap.get(3)) \n",
    "    frame_height = int(cap.get(4))\n",
    "    \n",
    "    base_landmarks = None\n",
    "    \n",
    "    start = time.time()\n",
    "    current_timestamp = None\n",
    "    video_name = video_path.split('/')[-1]\n",
    "    current_group = 1\n",
    "    while(cap.isOpened): \n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:        \n",
    "            break\n",
    "\n",
    "        count += 1\n",
    "\n",
    "        orig_image = frame #store frame\n",
    "        image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB) #convert frame to RGB\n",
    "        image = letterbox(image, (frame_width), stride=64, auto=True)[0]\n",
    "        image_ = image.copy()\n",
    "        image = transforms.ToTensor()(image)\n",
    "        image = torch.tensor(np.array([image.numpy()]))\n",
    "\n",
    "        image = image.to(device)  #convert image data to device\n",
    "        image = image.float() #convert image to float precision (cpu)      \n",
    "\n",
    "\n",
    "        image = image.cpu().squeeze().numpy().transpose((1, 2, 0))\n",
    "        \n",
    "        # this frame size works with yolov7, since we don't want to touch their model, we just resize the frame.\n",
    "        desired_width = 640\n",
    "        desired_height = 512\n",
    "        image = cv2.resize(image, (desired_width, desired_height), interpolation=cv2.INTER_LINEAR)\n",
    "        image = image[:desired_height, :desired_width]\n",
    "\n",
    "        # Convert the cropped image back to a torch.Tensor\n",
    "        image = torch.from_numpy(image.transpose((2, 0, 1))).unsqueeze(0).cuda()  \n",
    "\n",
    "        with torch.no_grad():  #get predictions\n",
    "            output_data, _ = model(image)\n",
    "\n",
    "        output_data = non_max_suppression_kpt(output_data,   #Apply non max suppression\n",
    "                                        0.25, # Conf. Threshold.\n",
    "                                        0.65, # IoU Threshold.\n",
    "                                        nc=model.yaml['nc'], # Number of classes.\n",
    "                                        nkpt=model.yaml['nkpt'], # Number of keypoints.\n",
    "                                        kpt_label=True)\n",
    "\n",
    "        output = output_to_keypoint(output_data)\n",
    "\n",
    "        im0 = image[0].permute(1, 2, 0) * 255 # Change format [b, c, h, w] to [h, w, c] for displaying the image.\n",
    "        im0 = im0.cpu().numpy().astype(np.uint8)\n",
    "\n",
    "        im0 = cv2.cvtColor(im0, cv2.COLOR_RGB2BGR) #reshape image format to (BGR)\n",
    "        gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "\n",
    "        for i, pose in enumerate(output_data):  # detections per image   \n",
    "            if len(output_data):  #check if no pose\n",
    "                for c in pose[:, 5].unique(): # Print results\n",
    "                    n = (pose[:, 5] == c).sum()  # detections per class\n",
    "\n",
    "                for det_index, (*xyxy, conf, cls) in enumerate(reversed(pose[:,:6])): #loop over poses for drawing on frame\n",
    "                    c = int(cls)  # integer class\n",
    "                    kpts = pose[det_index, 6:]\n",
    "                    label = None # if opt.hide_labels else (names[c] if opt.hide_conf else f'{names[c]} {conf:.2f}')\n",
    "                    plot_one_box_kpt(xyxy, im0, label=label, color=colors(c, True), \n",
    "                                    line_thickness=3,kpt_label=True, kpts=kpts, steps=3, \n",
    "                                    orig_shape=im0.shape[:2])\n",
    "\n",
    "\n",
    "        if count == separation:\n",
    "            # cv2.imshow(\"YOLOv7 Pose Estimation Demo\", im0)\n",
    "\n",
    "            if len(sequences[-1]) >= sequence_length:\n",
    "                sequences += [[]] # init new empty sequence\n",
    "            else:\n",
    "                # TODO: make sure that the landmarks stored are the desired ones\n",
    "                # Use position difference.\n",
    "                if len(output):\n",
    "                    \n",
    "                    sequences[-1] += [[video_name, current_group, cap.get(cv2.CAP_PROP_POS_MSEC), output[0]]]\n",
    "            count = 0\n",
    "            current_group += 1\n",
    "\n",
    "        # Press Q on keyboard to  exit\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"\\tfinished after {round(time.time() - start)}s\")\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../dataset_videos/video3.mp4', '../dataset_videos/video5.mp4', '../dataset_videos/video4.mp4', '../dataset_videos/video2.mp4', '../dataset_videos/video1.mp4']\n"
     ]
    }
   ],
   "source": [
    "# landmark retrieval\n",
    "videos = ['../dataset_videos/' + v for v in os.listdir('../dataset_videos')]\n",
    "dataset = []\n",
    "print(videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video #0 at '../dataset_videos/video3.mp4'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, v in enumerate(videos):\n",
    "    print(f\"Processing video #{i} at '{v}'\")\n",
    "    sequences = landmarks_sequence_for_video(v)\n",
    "    dataset += [sequences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this merges every set of landmarks. \n",
    "flatten_data = list(itertools.chain.from_iterable(dataset))\n",
    "flatten_data = list(itertools.chain.from_iterable(flatten_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns='video group timestamp sequence'.split(), data=flatten_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>video3.mp4</td>\n",
       "      <td>166.666667</td>\n",
       "      <td>[0.0, 0.0, 289.0726318359375, 264.521087646484...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>video3.mp4</td>\n",
       "      <td>366.666667</td>\n",
       "      <td>[0.0, 0.0, 290.0937805175781, 283.631774902343...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>video3.mp4</td>\n",
       "      <td>566.666667</td>\n",
       "      <td>[0.0, 0.0, 278.62213134765625, 278.41619873046...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>video3.mp4</td>\n",
       "      <td>766.666667</td>\n",
       "      <td>[0.0, 0.0, 270.9732360839844, 272.070831298828...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>video3.mp4</td>\n",
       "      <td>966.666667</td>\n",
       "      <td>[0.0, 0.0, 270.0237121582031, 279.718139648437...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        video   timestamp                                           sequence\n",
       "0  video3.mp4  166.666667  [0.0, 0.0, 289.0726318359375, 264.521087646484...\n",
       "1  video3.mp4  366.666667  [0.0, 0.0, 290.0937805175781, 283.631774902343...\n",
       "2  video3.mp4  566.666667  [0.0, 0.0, 278.62213134765625, 278.41619873046...\n",
       "3  video3.mp4  766.666667  [0.0, 0.0, 270.9732360839844, 272.070831298828...\n",
       "4  video3.mp4  966.666667  [0.0, 0.0, 270.0237121582031, 279.718139648437..."
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('dataset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
