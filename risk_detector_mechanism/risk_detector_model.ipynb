{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "b816e63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "import ast\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b5596d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../landmark_extraction_mechanism/labeled_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6ae623cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabel = pd.read_csv('../landmark_extraction_mechanism/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a2efa9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabel['label'] = dataset['Label']\n",
    "dataset = unlabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3f0266d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_float(num):\n",
    "    try:\n",
    "        float(num)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "3a3ada25",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['landmarks'] = dataset['landmarks'].apply(lambda arr: np.array([float(n) for n in arr.split() if is_float(n)]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "559d954b",
   "metadata": {},
   "source": [
    "### Dataset csv\n",
    "\n",
    "Each row contains a video (so we can label the data), also the timestamp of the frame where that sequence set of landmarks was captured and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "17567882",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Group the data by 'video' and 'group'\n",
    "grouped_data = dataset.groupby(['video', 'group'])\n",
    "\n",
    "# Define the sequence length\n",
    "sequence_length = 10\n",
    "\n",
    "# Create lists to store the sequences and labels\n",
    "sequences = []\n",
    "labels = []\n",
    "\n",
    "# Iterate over each group\n",
    "for group, data in grouped_data:\n",
    "    landmarks = data['landmarks'].tolist()\n",
    "    group_labels = data['label'].tolist()\n",
    "    \n",
    "    # Create sequences of landmarks\n",
    "    for i in range(len(landmarks) - sequence_length + 1):\n",
    "        sequence = landmarks[i:i+sequence_length]\n",
    "        sequences.append(sequence)\n",
    "        labels.append(group_labels[i+sequence_length-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "aaee55f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d688fb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "normalized_sequences = np.zeros_like(sequences)\n",
    "\n",
    "for i in range(sequences.shape[0]):\n",
    "    for j in range(sequences.shape[1]):\n",
    "        # Flatten the landmarks for each set within the sequence\n",
    "        landmarks_flattened = np.reshape(sequences[i, j], (-1, 1))\n",
    "        # Normalize the landmarks\n",
    "        landmarks_normalized = scaler.fit_transform(landmarks_flattened)\n",
    "        # Reshape the normalized landmarks back to the original shape\n",
    "        normalized_landmarks = np.reshape(landmarks_normalized, sequences[i, j].shape)\n",
    "        # Update the normalized landmarks in the sequences array\n",
    "        normalized_sequences[i, j] = normalized_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "7fe98765",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6f7d9ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(normalized_sequences, labels_encoded, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "2a103a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35, 10, 57)\n",
      "[1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 0 0 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(train_X.shape)\n",
    "print(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "b0a97cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_tensor = torch.Tensor(train_X)\n",
    "train_y_tensor = torch.Tensor(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "62ffefb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_X_tensor, train_y_tensor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "f3fee3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(hidden_size, 32)\n",
    "        self.fc2 = nn.Linear(32, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        x = self.dropout(h_n[-1])\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e00a466c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = train_X.shape[2]\n",
    "hidden_size = 64\n",
    "num_classes = 1\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Instantiate the model\n",
    "model = LSTMModel(input_size, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "cbd8f248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "e25c0432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.6634728312492371\n",
      "Epoch 2/10, Loss: 0.7621971964836121\n",
      "Epoch 3/10, Loss: 0.5117874145507812\n",
      "Epoch 4/10, Loss: 0.48220014572143555\n",
      "Epoch 5/10, Loss: 0.6094920635223389\n",
      "Epoch 6/10, Loss: 0.3894810974597931\n",
      "Epoch 7/10, Loss: 0.31822773814201355\n",
      "Epoch 8/10, Loss: 0.30327558517456055\n",
      "Epoch 9/10, Loss: 0.7119357585906982\n",
      "Epoch 10/10, Loss: 0.6103209853172302\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_dataloader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print the loss for every epoch\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "9652ae63",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X_tensor = torch.Tensor(test_X)\n",
    "test_y_tensor = torch.Tensor(test_y)\n",
    "\n",
    "test_dataset = TensorDataset(test_X_tensor, test_y_tensor)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)  # Set shuffle to False for evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "65d2c210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.67\n",
      "Precision: 0.44\n",
      "Recall: 0.67\n",
      "F1 Score: 0.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Create lists to store the predicted labels and ground truth labels\n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "\n",
    "# Iterate over the test data\n",
    "for inputs, labels in test_dataloader:\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # Get the predicted labels by taking the maximum probability\n",
    "    _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "    # Append the predicted labels and true labels to the lists\n",
    "    predicted_labels.extend(predicted.tolist())\n",
    "    true_labels.extend(labels.tolist())\n",
    "\n",
    "# Convert the predicted labels and true labels to numpy arrays\n",
    "predicted_labels = np.array(predicted_labels)\n",
    "true_labels = np.array(true_labels)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
    "recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "3fc0b1ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (lstm): LSTM(57, 64, batch_first=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (fc2): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
